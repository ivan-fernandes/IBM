{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2de014",
   "metadata": {},
   "source": [
    "# ApacheSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d12c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/30 12:35:16 WARN Utils: Your hostname, IFernandes resolves to a loopback address: 127.0.1.1; using 192.168.240.163 instead (on interface eth0)\n",
      "22/01/30 12:35:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/30 12:35:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/30 12:35:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95afe012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/29 20:30:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/29 20:30:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(100)) # saved in ApacheSpark memory - best solution\n",
    "rdd2 = range(100) # saved in local driver - worse solution for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced273eb",
   "metadata": {},
   "source": [
    "## Functional programming\n",
    "lambda functions, applying a function to a set of data. \\\n",
    "then parallelization takes place by taken sub samples of this list and processing them at different nodes/cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5abca24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functional programming\n",
    "rdd = sc.parallelize(range(100))\n",
    "rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e1da63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10), rdd.map(lambda x: x+1).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab6e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1,101)).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937375c",
   "metadata": {},
   "source": [
    "## RDD - Resilient Distributed Dataset and DataFrames\n",
    "\n",
    "ApacheSparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing cloudant from ApacheSpark\n",
    "cloudantdata = sparSession.read.format(\"com.cloudant.spark\").\\\n",
    "option(\"cloudant.host\", \"\"). \\\n",
    "option(\"cloudant.username\", \"\"). \\\n",
    "option(\"cloudant.password\", \"\"). \\\n",
    "load(\"device_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudantdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudantdata.createOrReplaceTempView(\"device_name\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM device_name\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348eecd",
   "metadata": {},
   "source": [
    "## Math and Statistics on ApacheSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b36117a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7bfecc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rddX = sc.parallelize(random.sample(list(range(100)),100))\n",
    "rddY = sc.parallelize(random.sample(list(range(100)),100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ae5f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5 49.5\n",
      "49.5\n"
     ]
    }
   ],
   "source": [
    "meanX = rddX.sum()/float(rddX.count())\n",
    "meanY = rddY.sum()/float(rddY.count())\n",
    "print (meanX, rddX.mean())\n",
    "print (meanY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7984836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation\n",
    "import math\n",
    "stdd = math.sqrt(rdd.map(lambda x: pow(x-meanX,2)).sum()/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "61d0408c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7997599759975997"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skewness\n",
    "rdd.map(lambda x: pow(x-meanX,3)/pow(stdd,3)).sum()/n\n",
    "\n",
    "# kurtosis - shape of the data and outliers content within the data\n",
    "kurt = rdd.map(lambda x: pow(x-meanX,4)/pow(stdd,4)).sum()/n\n",
    "kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "71c8b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.86607004772212 28.86607004772212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04537653765376538"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covariance and correlation\n",
    "n = rddXY.count()\n",
    "## covariance\n",
    "rddXY = rddX.zip(rddY)\n",
    "covXY = rddXY.map(lambda x_y : (x_y[0]-meanX)*(x_y[1]-meanY)).sum()/n\n",
    "covXY\n",
    "\n",
    "## correlation: corr = covarXY / (stdX * stdY)\n",
    "from math import sqrt\n",
    "stdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\n",
    "stdY = sqrt(rddY.map(lambda x : pow(x-meanY,2)).sum()/n)\n",
    "print (stdX, stdY)\n",
    "\n",
    "corrXY = covXY / (stdX*stdY)\n",
    "corrXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0524bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation matrix\n",
    "column1 = sc.parallelize(range(100))\n",
    "column2 = sc.parallelize(range(100,200))\n",
    "column3 = sc.parallelize(list(reversed(range(100))))\n",
    "import random\n",
    "column4 = sc.parallelize(random.sample(range(100),100))\n",
    "\n",
    "data = column1.zip(column2).zip(column3).zip(column4).map(lambda a_b_c_d : (a_b_c_d[0][0][0],a_b_c_d[0][0][1],a_b_c_d[0][1],a_b_c_d[1]).map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]])) \n",
    "#data.map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]]).take(10)\n",
    "from pyspark.mllib.stat import Statistics\n",
    "print(Statistics.corr(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "78f9ecc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          d1        d2\n",
       " d1  1.000000  0.709273\n",
       " d2  0.709273  1.000000,\n",
       "           d1        d2\n",
       " d1  9.166667  4.055556\n",
       " d2  4.055556  3.566667)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[1,2,3,4,5,6,7,8,9,10]\n",
    "b = [7,6,5,4,5,6,7,8,9,10]\n",
    "import pandas as pd\n",
    "di = {\"d1\":a,\"d2\":b}\n",
    "data = pd.DataFrame(di)\n",
    "data.corr(), data.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f7b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd912898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "072b27f8",
   "metadata": {},
   "source": [
    "## ApacheSpark run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60fcf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/29 14:05:34 WARN Utils: Your hostname, IFernandes resolves to a loopback address: 127.0.1.1; using 192.168.240.163 instead (on interface eth0)\n",
      "22/01/29 14:05:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/29 14:05:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/29 14:05:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14186988\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b651a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
